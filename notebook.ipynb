{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gmum/natural-language-processing-classes/blob/master/lab-9-unsupervised-lm-training/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xl-tY1QsFKex"
   },
   "source": [
    "# Training unsupervised Language Models\n",
    "\n",
    "## Excercise\n",
    "\n",
    "Train an Language Model that obtains at least **111** perplexity on the test set.\n",
    "\n",
    "Remember to:\n",
    "-  use gradient clipping with value 0.25\n",
    "-  use hidden state from previous batch in next batch, to keep the information longer. To do this, instead of initializing the hidden state with 0 each batch, we detach the hidden state from how it was previously produced. If we didn't, the model would try backpropagating all the way to start of the dataset. Use repackage_hidden to deal with that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4UWE49iajKZ8",
    "outputId": "77285be4-5bcc-449c-ec68-e7e9d493d1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtorch-0.4.1-cp36-cp36m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f820930>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU support!\n",
    "\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Use dataset from https://drive.google.com/drive/folders/1e-BUHYY61Vy9AGNuh2nungslO-mYuVox?usp=sharing\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2cnfYyip3Mj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "aer\n",
      "banknote\n",
      "berlitz\n",
      "calloway\n",
      "centrust\n",
      "cluett\n",
      "fromstein\n",
      "gitano\n",
      "guterman\n",
      "hydro-quebec\n",
      "ipo\n",
      "kia\n",
      "memotec\n",
      "mlx\n",
      "nahb\n",
      "punts\n",
      "rake\n",
      "regatta\n",
      "rubens\n",
      "sim\n",
      "snack-food\n",
      "ssangyong\n",
      "swapo\n",
      "wachter\n",
      "<eos>\n",
      "pierre\n",
      "<unk>\n",
      "N\n",
      "years\n",
      "old\n",
      "will\n",
      "join\n",
      "the\n",
      "board\n",
      "as\n",
      "a\n",
      "nonexecutive\n",
      "director\n",
      "nov.\n",
      "N\n",
      "<eos>\n",
      "mr.\n",
      "<unk>\n",
      "is\n",
      "chairman\n",
      "of\n",
      "<unk>\n",
      "n.v.\n",
      "the\n",
      "dutch\n"
     ]
    }
   ],
   "source": [
    "class Dictionary(object):\n",
    "    \"\"\"Build word2idx and idx2word from Corpus(train/val/test)\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {} # word: index\n",
    "        self.idx2word = [] # position(index): word\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Create/Update word2idx and idx2word\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    \"\"\"Corpus Tokenizer\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'ptb.train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'ptb.valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'ptb.test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                # line to list of token + eos\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids\n",
    "      \n",
    "      \n",
    "def batchify(data, bsz, verbose=False):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    # See https://pytorch.org/docs/stable/torch.html#torch.narrow for more explaination\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    # .t() is transposition: https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "    # the contiguous function doesn't affect your target tensor at all, it just \n",
    "    # makes sure that it is stored in a contiguous chunk of memory.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    if verbose:\n",
    "      print(data.size())\n",
    "      for el in data[:50,0]:\n",
    "        print(corpus.dictionary.idx2word[el.item()])\n",
    "      \n",
    "#     data = data.cuda()\n",
    "    return data\n",
    "\n",
    "# use path to where you store the datasets\n",
    "corpus = Corpus(os.getcwd()+'/data')\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, batch_size, verbose=True)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IriDBdrYrh40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (encoder): Embedding(10000, 150)\n",
      "  (lstm): LSTM(150, 150, dropout=0.2)\n",
      "  (decoder): Linear(in_features=150, out_features=10000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # Token2Embeddings\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "#         return (torch.zeros(self.nlayers, bsz, self.nhid).cuda(),\n",
    "#                 torch.zeros(self.nlayers, bsz, self.nhid).cuda())\n",
    "            return (torch.zeros(self.nlayers, bsz, self.nhid),\n",
    "                torch.zeros(self.nlayers, bsz, self.nhid))\n",
    "\n",
    "model = LSTMModel(ntokens, 150, 150, 1, 0.2)\n",
    "# model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4pz0dFN00-I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 1549 batches | lr 20.00 | ms/batch 275.15 | loss  5.06 | ppl   157.26\n",
      "| epoch   1 |   200/ 1549 batches | lr 20.00 | ms/batch 236.07 | loss  5.07 | ppl   159.61\n",
      "| epoch   1 |   300/ 1549 batches | lr 20.00 | ms/batch 264.35 | loss  5.17 | ppl   176.31\n",
      "| epoch   1 |   400/ 1549 batches | lr 20.00 | ms/batch 234.83 | loss  5.08 | ppl   160.34\n",
      "| epoch   1 |   500/ 1549 batches | lr 20.00 | ms/batch 239.30 | loss  5.08 | ppl   160.89\n",
      "| epoch   1 |   600/ 1549 batches | lr 20.00 | ms/batch 233.72 | loss  5.10 | ppl   163.74\n",
      "| epoch   1 |   700/ 1549 batches | lr 20.00 | ms/batch 299.03 | loss  5.08 | ppl   160.66\n",
      "| epoch   1 |   800/ 1549 batches | lr 20.00 | ms/batch 247.53 | loss  5.05 | ppl   156.77\n",
      "| epoch   1 |   900/ 1549 batches | lr 20.00 | ms/batch 225.59 | loss  4.95 | ppl   141.15\n",
      "| epoch   1 |  1000/ 1549 batches | lr 20.00 | ms/batch 257.95 | loss  5.00 | ppl   147.88\n",
      "| epoch   1 |  1100/ 1549 batches | lr 20.00 | ms/batch 235.32 | loss  5.03 | ppl   152.69\n",
      "| epoch   1 |  1200/ 1549 batches | lr 20.00 | ms/batch 223.86 | loss  5.06 | ppl   157.74\n",
      "| epoch   1 |  1300/ 1549 batches | lr 20.00 | ms/batch 260.15 | loss  4.87 | ppl   130.17\n",
      "| epoch   1 |  1400/ 1549 batches | lr 20.00 | ms/batch 260.20 | loss  4.91 | ppl   135.88\n",
      "| epoch   1 |  1500/ 1549 batches | lr 20.00 | ms/batch 260.08 | loss  4.86 | ppl   128.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 396.77s | valid loss  4.99 | valid ppl   146.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/ 1549 batches | lr 20.00 | ms/batch 220.09 | loss  4.87 | ppl   130.11\n",
      "| epoch   2 |   200/ 1549 batches | lr 20.00 | ms/batch 218.01 | loss  4.86 | ppl   129.35\n",
      "| epoch   2 |   300/ 1549 batches | lr 20.00 | ms/batch 239.21 | loss  4.98 | ppl   145.72\n",
      "| epoch   2 |   400/ 1549 batches | lr 20.00 | ms/batch 275.71 | loss  4.87 | ppl   130.33\n",
      "| epoch   2 |   500/ 1549 batches | lr 20.00 | ms/batch 304.43 | loss  4.88 | ppl   131.59\n",
      "| epoch   2 |   600/ 1549 batches | lr 20.00 | ms/batch 296.15 | loss  4.91 | ppl   135.79\n",
      "| epoch   2 |   700/ 1549 batches | lr 20.00 | ms/batch 225.46 | loss  4.89 | ppl   133.59\n",
      "| epoch   2 |   800/ 1549 batches | lr 20.00 | ms/batch 219.20 | loss  4.88 | ppl   131.37\n",
      "| epoch   2 |   900/ 1549 batches | lr 20.00 | ms/batch 219.68 | loss  4.79 | ppl   119.91\n",
      "| epoch   2 |  1000/ 1549 batches | lr 20.00 | ms/batch 220.86 | loss  4.84 | ppl   125.86\n",
      "| epoch   2 |  1100/ 1549 batches | lr 20.00 | ms/batch 221.17 | loss  4.88 | ppl   131.45\n",
      "| epoch   2 |  1200/ 1549 batches | lr 20.00 | ms/batch 219.44 | loss  4.92 | ppl   136.63\n",
      "| epoch   2 |  1300/ 1549 batches | lr 20.00 | ms/batch 218.78 | loss  4.70 | ppl   110.07\n",
      "| epoch   2 |  1400/ 1549 batches | lr 20.00 | ms/batch 217.72 | loss  4.77 | ppl   117.41\n",
      "| epoch   2 |  1500/ 1549 batches | lr 20.00 | ms/batch 219.62 | loss  4.72 | ppl   111.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 373.30s | valid loss  4.91 | valid ppl   136.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/ 1549 batches | lr 20.00 | ms/batch 222.08 | loss  4.75 | ppl   115.74\n",
      "| epoch   3 |   200/ 1549 batches | lr 20.00 | ms/batch 220.17 | loss  4.74 | ppl   114.91\n",
      "| epoch   3 |   300/ 1549 batches | lr 20.00 | ms/batch 220.63 | loss  4.87 | ppl   129.71\n",
      "| epoch   3 |   400/ 1549 batches | lr 20.00 | ms/batch 220.68 | loss  4.75 | ppl   116.04\n",
      "| epoch   3 |   500/ 1549 batches | lr 20.00 | ms/batch 219.30 | loss  4.75 | ppl   115.99\n",
      "| epoch   3 |   600/ 1549 batches | lr 20.00 | ms/batch 221.49 | loss  4.80 | ppl   121.09\n",
      "| epoch   3 |   700/ 1549 batches | lr 20.00 | ms/batch 219.61 | loss  4.78 | ppl   119.39\n",
      "| epoch   3 |   800/ 1549 batches | lr 20.00 | ms/batch 222.91 | loss  4.76 | ppl   117.19\n",
      "| epoch   3 |   900/ 1549 batches | lr 20.00 | ms/batch 219.68 | loss  4.67 | ppl   107.03\n",
      "| epoch   3 |  1000/ 1549 batches | lr 20.00 | ms/batch 222.11 | loss  4.74 | ppl   114.06\n",
      "| epoch   3 |  1100/ 1549 batches | lr 20.00 | ms/batch 221.06 | loss  4.78 | ppl   119.08\n",
      "| epoch   3 |  1200/ 1549 batches | lr 20.00 | ms/batch 220.90 | loss  4.82 | ppl   124.42\n",
      "| epoch   3 |  1300/ 1549 batches | lr 20.00 | ms/batch 221.06 | loss  4.60 | ppl    99.62\n",
      "| epoch   3 |  1400/ 1549 batches | lr 20.00 | ms/batch 222.61 | loss  4.66 | ppl   105.19\n",
      "| epoch   3 |  1500/ 1549 batches | lr 20.00 | ms/batch 222.61 | loss  4.63 | ppl   102.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 351.66s | valid loss  4.87 | valid ppl   130.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/ 1549 batches | lr 20.00 | ms/batch 233.14 | loss  4.67 | ppl   106.33\n",
      "| epoch   4 |   200/ 1549 batches | lr 20.00 | ms/batch 240.35 | loss  4.66 | ppl   105.21\n",
      "| epoch   4 |   300/ 1549 batches | lr 20.00 | ms/batch 219.02 | loss  4.79 | ppl   120.20\n",
      "| epoch   4 |   400/ 1549 batches | lr 20.00 | ms/batch 219.29 | loss  4.67 | ppl   106.25\n",
      "| epoch   4 |   500/ 1549 batches | lr 20.00 | ms/batch 220.48 | loss  4.67 | ppl   106.76\n",
      "| epoch   4 |   600/ 1549 batches | lr 20.00 | ms/batch 222.52 | loss  4.71 | ppl   111.42\n",
      "| epoch   4 |   700/ 1549 batches | lr 20.00 | ms/batch 219.13 | loss  4.70 | ppl   109.67\n",
      "| epoch   4 |   800/ 1549 batches | lr 20.00 | ms/batch 221.73 | loss  4.69 | ppl   108.33\n",
      "| epoch   4 |   900/ 1549 batches | lr 20.00 | ms/batch 221.96 | loss  4.60 | ppl    99.21\n",
      "| epoch   4 |  1000/ 1549 batches | lr 20.00 | ms/batch 221.55 | loss  4.66 | ppl   105.80\n",
      "| epoch   4 |  1100/ 1549 batches | lr 20.00 | ms/batch 220.87 | loss  4.71 | ppl   110.56\n",
      "| epoch   4 |  1200/ 1549 batches | lr 20.00 | ms/batch 221.93 | loss  4.75 | ppl   115.48\n",
      "| epoch   4 |  1300/ 1549 batches | lr 20.00 | ms/batch 221.58 | loss  4.53 | ppl    92.36\n",
      "| epoch   4 |  1400/ 1549 batches | lr 20.00 | ms/batch 222.37 | loss  4.59 | ppl    98.62\n",
      "| epoch   4 |  1500/ 1549 batches | lr 20.00 | ms/batch 222.48 | loss  4.56 | ppl    95.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 354.70s | valid loss  4.84 | valid ppl   126.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/ 1549 batches | lr 20.00 | ms/batch 223.18 | loss  4.61 | ppl   100.08\n",
      "| epoch   5 |   200/ 1549 batches | lr 20.00 | ms/batch 220.89 | loss  4.60 | ppl    99.39\n",
      "| epoch   5 |   300/ 1549 batches | lr 20.00 | ms/batch 221.57 | loss  4.73 | ppl   113.47\n",
      "| epoch   5 |   400/ 1549 batches | lr 20.00 | ms/batch 235.89 | loss  4.59 | ppl    98.91\n",
      "| epoch   5 |   500/ 1549 batches | lr 20.00 | ms/batch 226.68 | loss  4.61 | ppl   100.28\n",
      "| epoch   5 |   600/ 1549 batches | lr 20.00 | ms/batch 219.72 | loss  4.65 | ppl   104.26\n",
      "| epoch   5 |   700/ 1549 batches | lr 20.00 | ms/batch 218.40 | loss  4.64 | ppl   103.34\n",
      "| epoch   5 |   800/ 1549 batches | lr 20.00 | ms/batch 219.95 | loss  4.62 | ppl   101.59\n",
      "| epoch   5 |   900/ 1549 batches | lr 20.00 | ms/batch 218.81 | loss  4.54 | ppl    93.36\n",
      "| epoch   5 |  1000/ 1549 batches | lr 20.00 | ms/batch 219.47 | loss  4.60 | ppl    99.74\n",
      "| epoch   5 |  1100/ 1549 batches | lr 20.00 | ms/batch 220.73 | loss  4.65 | ppl   104.33\n",
      "| epoch   5 |  1200/ 1549 batches | lr 20.00 | ms/batch 218.91 | loss  4.70 | ppl   109.75\n",
      "| epoch   5 |  1300/ 1549 batches | lr 20.00 | ms/batch 219.72 | loss  4.47 | ppl    87.07\n",
      "| epoch   5 |  1400/ 1549 batches | lr 20.00 | ms/batch 220.12 | loss  4.54 | ppl    93.35\n",
      "| epoch   5 |  1500/ 1549 batches | lr 20.00 | ms/batch 219.84 | loss  4.51 | ppl    90.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 352.33s | valid loss  4.83 | valid ppl   125.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/ 1549 batches | lr 20.00 | ms/batch 225.29 | loss  4.56 | ppl    95.54\n",
      "| epoch   6 |   200/ 1549 batches | lr 20.00 | ms/batch 219.76 | loss  4.55 | ppl    94.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   300/ 1549 batches | lr 20.00 | ms/batch 219.48 | loss  4.67 | ppl   106.99\n",
      "| epoch   6 |   400/ 1549 batches | lr 20.00 | ms/batch 219.83 | loss  4.55 | ppl    94.43\n",
      "| epoch   6 |   500/ 1549 batches | lr 20.00 | ms/batch 220.95 | loss  4.56 | ppl    95.35\n",
      "| epoch   6 |   600/ 1549 batches | lr 20.00 | ms/batch 228.65 | loss  4.59 | ppl    98.96\n",
      "| epoch   6 |   700/ 1549 batches | lr 20.00 | ms/batch 221.69 | loss  4.59 | ppl    98.57\n",
      "| epoch   6 |   800/ 1549 batches | lr 20.00 | ms/batch 222.23 | loss  4.58 | ppl    97.25\n",
      "| epoch   6 |   900/ 1549 batches | lr 20.00 | ms/batch 221.24 | loss  4.49 | ppl    88.75\n",
      "| epoch   6 |  1000/ 1549 batches | lr 20.00 | ms/batch 221.11 | loss  4.55 | ppl    94.78\n",
      "| epoch   6 |  1100/ 1549 batches | lr 20.00 | ms/batch 220.32 | loss  4.61 | ppl   100.20\n",
      "| epoch   6 |  1200/ 1549 batches | lr 20.00 | ms/batch 224.00 | loss  4.65 | ppl   104.56\n",
      "| epoch   6 |  1300/ 1549 batches | lr 20.00 | ms/batch 221.14 | loss  4.43 | ppl    84.15\n",
      "| epoch   6 |  1400/ 1549 batches | lr 20.00 | ms/batch 219.60 | loss  4.48 | ppl    88.60\n",
      "| epoch   6 |  1500/ 1549 batches | lr 20.00 | ms/batch 219.93 | loss  4.47 | ppl    87.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 352.10s | valid loss  4.83 | valid ppl   125.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/ 1549 batches | lr 20.00 | ms/batch 223.64 | loss  4.52 | ppl    91.62\n",
      "| epoch   7 |   200/ 1549 batches | lr 20.00 | ms/batch 220.85 | loss  4.51 | ppl    91.20\n",
      "| epoch   7 |   300/ 1549 batches | lr 20.00 | ms/batch 225.77 | loss  4.64 | ppl   103.38\n",
      "| epoch   7 |   400/ 1549 batches | lr 20.00 | ms/batch 219.06 | loss  4.50 | ppl    90.34\n",
      "| epoch   7 |   500/ 1549 batches | lr 20.00 | ms/batch 219.86 | loss  4.52 | ppl    91.70\n",
      "| epoch   7 |   600/ 1549 batches | lr 20.00 | ms/batch 220.58 | loss  4.56 | ppl    95.34\n",
      "| epoch   7 |   700/ 1549 batches | lr 20.00 | ms/batch 220.39 | loss  4.55 | ppl    94.75\n",
      "| epoch   7 |   800/ 1549 batches | lr 20.00 | ms/batch 220.07 | loss  4.54 | ppl    93.89\n",
      "| epoch   7 |   900/ 1549 batches | lr 20.00 | ms/batch 220.22 | loss  4.45 | ppl    85.99\n",
      "| epoch   7 |  1000/ 1549 batches | lr 20.00 | ms/batch 219.89 | loss  4.52 | ppl    92.20\n",
      "| epoch   7 |  1100/ 1549 batches | lr 20.00 | ms/batch 220.84 | loss  4.57 | ppl    96.14\n",
      "| epoch   7 |  1200/ 1549 batches | lr 20.00 | ms/batch 221.12 | loss  4.61 | ppl   100.93\n",
      "| epoch   7 |  1300/ 1549 batches | lr 20.00 | ms/batch 222.40 | loss  4.39 | ppl    80.32\n",
      "| epoch   7 |  1400/ 1549 batches | lr 20.00 | ms/batch 227.99 | loss  4.45 | ppl    85.69\n",
      "| epoch   7 |  1500/ 1549 batches | lr 20.00 | ms/batch 220.49 | loss  4.43 | ppl    83.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 352.00s | valid loss  4.83 | valid ppl   125.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/ 1549 batches | lr 20.00 | ms/batch 222.65 | loss  4.49 | ppl    88.82\n",
      "| epoch   8 |   200/ 1549 batches | lr 20.00 | ms/batch 219.87 | loss  4.47 | ppl    87.37\n",
      "| epoch   8 |   300/ 1549 batches | lr 20.00 | ms/batch 219.63 | loss  4.60 | ppl    99.49\n",
      "| epoch   8 |   400/ 1549 batches | lr 20.00 | ms/batch 220.26 | loss  4.47 | ppl    87.66\n",
      "| epoch   8 |   500/ 1549 batches | lr 20.00 | ms/batch 220.91 | loss  4.48 | ppl    88.54\n",
      "| epoch   8 |   600/ 1549 batches | lr 20.00 | ms/batch 222.53 | loss  4.52 | ppl    92.07\n",
      "| epoch   8 |   700/ 1549 batches | lr 20.00 | ms/batch 221.15 | loss  4.52 | ppl    91.68\n",
      "| epoch   8 |   800/ 1549 batches | lr 20.00 | ms/batch 221.38 | loss  4.51 | ppl    90.84\n",
      "| epoch   8 |   900/ 1549 batches | lr 20.00 | ms/batch 222.00 | loss  4.42 | ppl    83.41\n",
      "| epoch   8 |  1000/ 1549 batches | lr 20.00 | ms/batch 224.03 | loss  4.49 | ppl    88.97\n",
      "| epoch   8 |  1100/ 1549 batches | lr 20.00 | ms/batch 221.94 | loss  4.54 | ppl    94.06\n",
      "| epoch   8 |  1200/ 1549 batches | lr 20.00 | ms/batch 222.00 | loss  4.58 | ppl    97.88\n",
      "| epoch   8 |  1300/ 1549 batches | lr 20.00 | ms/batch 222.09 | loss  4.36 | ppl    78.39\n",
      "| epoch   8 |  1400/ 1549 batches | lr 20.00 | ms/batch 221.71 | loss  4.42 | ppl    82.70\n",
      "| epoch   8 |  1500/ 1549 batches | lr 20.00 | ms/batch 221.79 | loss  4.40 | ppl    81.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 352.20s | valid loss  4.82 | valid ppl   123.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/ 1549 batches | lr 20.00 | ms/batch 223.33 | loss  4.47 | ppl    86.95\n",
      "| epoch   9 |   200/ 1549 batches | lr 20.00 | ms/batch 221.64 | loss  4.45 | ppl    85.35\n",
      "| epoch   9 |   300/ 1549 batches | lr 20.00 | ms/batch 221.73 | loss  4.57 | ppl    96.52\n",
      "| epoch   9 |   400/ 1549 batches | lr 20.00 | ms/batch 221.13 | loss  4.44 | ppl    84.84\n",
      "| epoch   9 |   500/ 1549 batches | lr 20.00 | ms/batch 220.56 | loss  4.45 | ppl    85.81\n",
      "| epoch   9 |   600/ 1549 batches | lr 20.00 | ms/batch 220.78 | loss  4.49 | ppl    89.34\n",
      "| epoch   9 |   700/ 1549 batches | lr 20.00 | ms/batch 220.93 | loss  4.50 | ppl    89.85\n",
      "| epoch   9 |   800/ 1549 batches | lr 20.00 | ms/batch 223.72 | loss  4.48 | ppl    88.31\n",
      "| epoch   9 |   900/ 1549 batches | lr 20.00 | ms/batch 221.70 | loss  4.40 | ppl    81.50\n",
      "| epoch   9 |  1000/ 1549 batches | lr 20.00 | ms/batch 221.36 | loss  4.47 | ppl    86.99\n",
      "| epoch   9 |  1100/ 1549 batches | lr 20.00 | ms/batch 220.96 | loss  4.52 | ppl    91.63\n",
      "| epoch   9 |  1200/ 1549 batches | lr 20.00 | ms/batch 219.86 | loss  4.56 | ppl    95.50\n",
      "| epoch   9 |  1300/ 1549 batches | lr 20.00 | ms/batch 220.60 | loss  4.34 | ppl    76.76\n",
      "| epoch   9 |  1400/ 1549 batches | lr 20.00 | ms/batch 220.56 | loss  4.40 | ppl    81.23\n",
      "| epoch   9 |  1500/ 1549 batches | lr 20.00 | ms/batch 219.93 | loss  4.37 | ppl    78.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 351.52s | valid loss  4.82 | valid ppl   124.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/ 1549 batches | lr 5.00 | ms/batch 223.09 | loss  4.43 | ppl    83.59\n",
      "| epoch  10 |   200/ 1549 batches | lr 5.00 | ms/batch 219.55 | loss  4.37 | ppl    79.40\n",
      "| epoch  10 |   300/ 1549 batches | lr 5.00 | ms/batch 220.50 | loss  4.48 | ppl    88.29\n",
      "| epoch  10 |   400/ 1549 batches | lr 5.00 | ms/batch 221.58 | loss  4.32 | ppl    75.03\n",
      "| epoch  10 |   500/ 1549 batches | lr 5.00 | ms/batch 222.80 | loss  4.32 | ppl    75.31\n",
      "| epoch  10 |   600/ 1549 batches | lr 5.00 | ms/batch 220.76 | loss  4.35 | ppl    77.11\n",
      "| epoch  10 |   700/ 1549 batches | lr 5.00 | ms/batch 222.71 | loss  4.35 | ppl    77.36\n",
      "| epoch  10 |   800/ 1549 batches | lr 5.00 | ms/batch 219.73 | loss  4.31 | ppl    74.68\n",
      "| epoch  10 |   900/ 1549 batches | lr 5.00 | ms/batch 219.70 | loss  4.21 | ppl    67.20\n",
      "| epoch  10 |  1000/ 1549 batches | lr 5.00 | ms/batch 220.49 | loss  4.26 | ppl    71.08\n",
      "| epoch  10 |  1100/ 1549 batches | lr 5.00 | ms/batch 219.70 | loss  4.30 | ppl    73.79\n",
      "| epoch  10 |  1200/ 1549 batches | lr 5.00 | ms/batch 220.26 | loss  4.32 | ppl    75.50\n",
      "| epoch  10 |  1300/ 1549 batches | lr 5.00 | ms/batch 221.39 | loss  4.08 | ppl    59.36\n",
      "| epoch  10 |  1400/ 1549 batches | lr 5.00 | ms/batch 219.71 | loss  4.14 | ppl    62.58\n",
      "| epoch  10 |  1500/ 1549 batches | lr 5.00 | ms/batch 220.78 | loss  4.10 | ppl    60.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 351.08s | valid loss  4.71 | valid ppl   110.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.66 | test ppl   105.88\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion.cuda()\n",
    "seq_len = 30\n",
    "log_interval = 100\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == torch.Tensor:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    s_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+s_len])\n",
    "    target = Variable(source[i+1:i+1+s_len].view(-1))\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    \"\"\"compute total loss on data_source dataset\"\"\"\n",
    "  \n",
    "    model.eval() # Turn on evaluation mode which disables dropout.\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "      #run model on validation data and count loss\n",
    "      #lstm return vector of probab which wrod can be next for all words\n",
    "      #calc loss for lstm output and real probability of REAL next word\n",
    "      data, targets = get_batch(data_source, i)\n",
    "      output, hidden = model(data, hidden)\n",
    "      output_flat = output.view(-1, ntokens)\n",
    "      total_loss += len(data) * criterion(output_flat, targets).data\n",
    "      hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_len, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = 20\n",
    "best_val_loss = None\n",
    "# epochs = 40\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a sentance of 17 words, first 2 words are `consumers` and `kia`, the rest will be created by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consumers', 'kia', '<unk>', '<unk>', '<eos>', 'the', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "init_line = [2211,11]\n",
    "detoken = []\n",
    "for first_word_indx in init_line:\n",
    "    detoken.append(corpus.dictionary.idx2word[first_word_indx])\n",
    "gen_text_len = 15\n",
    "data_source = torch.LongTensor([[init_line[0]]])\n",
    "model.eval() # Turn on evaluation mode which disables dropout.\n",
    "ntokens = len(corpus.dictionary)\n",
    "hidden = model.init_hidden(1)\n",
    "output, output_flat = None, list()\n",
    "for i in range(1, len(init_line)):\n",
    "    data = Variable(data_source)\n",
    "    output, hidden = model(data, hidden)\n",
    "    output_flat.append(output.view(-1, ntokens))\n",
    "    hidden = repackage_hidden(hidden)\n",
    "    data_source = torch.LongTensor([[init_line[i]]])\n",
    "    \n",
    "for i in range(0, gen_text_len):\n",
    "#     print(data_source)\n",
    "    data = Variable(data_source)\n",
    "#     print(data)\n",
    "    output, hidden = model(data, hidden)\n",
    "    output_flat.append(output.view(-1, ntokens))\n",
    "    hidden = repackage_hidden(hidden)\n",
    "    \n",
    "    probs = list(output_flat[-1][0].detach().numpy())\n",
    "    max_indx = probs.index(max(probs))\n",
    "    detoken.append(corpus.dictionary.idx2word[max_indx])\n",
    "#     print(max_indx)\n",
    "    data_source = torch.LongTensor([[max_indx]])\n",
    "\n",
    "\n",
    "# for line in output_flat:\n",
    "#     probs = list(line.detach().numpy())\n",
    "#     max_indx = probs.index(max(probs))\n",
    "#     detoken.append(corpus.dictionary.idx2word[max_indx])\n",
    "\n",
    "print(detoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_flat[-1][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.4186041 , -0.42088753, -0.71642345, ..., -0.43088904,\n",
       "       -1.8156554 ,  0.11258146], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output_flat[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = list(output_flat[0].detach().numpy())\n",
    "max_indx = probs.index(max(probs))\n",
    "max_indx"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "lab-9-unsupervised-lm-training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
